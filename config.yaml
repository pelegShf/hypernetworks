train:
  lr: 3e-4
  weight_decay: 1e-4
  batch_size: 256
  epochs: 35
  gamma: 0.9
  n_classes: 10
  seed: 42
  normalize: 1
  dataset: "mnist"

model:
  layer_dims: [784,516, 256,128, 10]
  emb_dim: 64
  cnn_cond: true
  clip_grad: 0.7
  data_dir: "./Data/mnist"
  dropout: 0.3

log:
  interval: 100
  run_dir: runs/hypernet
  verbose: false


testing:
  batch_size: 64

misc:
  workers: 8